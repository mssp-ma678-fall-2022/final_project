---
title: "Final Project"
author: "Zhi Tu"
output:
  pdf_document: 
    latex_engine : xelatex
  html_document: default
date: '2022-11-18'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse, magrittr, ggplot2, lme4, rstanarm, sjPlot, lattice, merTools, performance, knitr, gridExtra, kableExtra)

```

## Read data

Download the data from Kaggle: <https://www.kaggle.com/datasets/josephgutstadt/data-jobs>.

```{r include=FALSE}
all_jobs <- read.csv("all_jobs.csv")
```

I got this data from the Kaggle site that contains data science related jobs posting on the Glassdoor website. It gives job descriptions, companies information and the estimated salary range. I would like to find out the reason behind different salaries with different companies. That is to say, I wonder how the location, the company background and the requirement for job applicant affect the predictive salary in data science related jobs. If I can build a model based on the information in this datasets. Individuals then are able to get a sense of the estimated salary paying for their skills and preferred working locations, etc.

Most information is contained in the job descriptions. So I would like to get key words from the job descriptions posted by companies. The first thing I did is to search for required or recommended skills for the job applicants. For data science related works, it is reasonable to assume some kind of programming languages, such as Python, R, and SQL. At the same time, some popular tools such as spark, excel and AWS. These keywords are valued as 1 if it's contained in the job description; otherwise, it's 0. As a result, we got columns consist of 0s and 1s for each keywords, which then can be used as predictors in the regression model.

## Data cleaning

```{r include=FALSE, warning=FALSE}
## search if language and R are in the same sentence
all_jobs %<>% mutate(R=ifelse(grepl("language[^\\.!?]*R", Job.Description),1,0))

## search for all kinds of keywords 
all_jobs %<>% mutate(python=ifelse(str_detect(all_jobs$Job.Description,regex("python", ignore_case = T)),1,0))
all_jobs %<>% mutate(aws=ifelse(str_detect(all_jobs$Job.Description,regex("AWS", ignore_case = F)),1,0))
all_jobs %<>% mutate(sql=ifelse(str_detect(all_jobs$Job.Description,regex("SQL", ignore_case = F)),1,0))
all_jobs %<>% mutate(spark=ifelse(str_detect(all_jobs$Job.Description,regex("spark", ignore_case = T)),1,0))
all_jobs %<>% mutate(excel=ifelse(str_detect(all_jobs$Job.Description,regex("excel", ignore_case = T)),1,0))
all_jobs %<>% mutate(seniority=ifelse(str_detect(all_jobs$Job.Title,regex("senior", ignore_case = T)),1,0))

```

We need to clean the Job Estimated Salary column. The original column is a string that contain the range of predicted salary. So we first extract the two ends of the salary range. Then, I planned to take the average on the estimated salary and use it as the outcome of the regression model; and set the other columns as the predictors. Observed that there are -1 values in these columns, I mutate these values to Unknown or Not Applicable by examining the classifications.

```{r include=FALSE, warning=FALSE}
## clean the string estimated salary column
all_jobs %<>% separate(Salary.Estimate, c("bottom", "top"), sep = "-")
regexp <- "[[:digit:]]+"
all_jobs %<>% mutate(bottom = as.numeric(str_extract(bottom, regexp)))
all_jobs %<>% mutate(top = as.numeric(str_extract(top, regexp)))
all_jobs %<>% mutate(median=(bottom+top)/2, .before=top)

## set -1 to unknown in company size
all_jobs %<>% mutate(Size=ifelse(Size==-1,"Unknown",Size))

## set -1 to Unknown / Non-Applicable in company revenue
all_jobs %<>% mutate(Revenue=ifelse(Revenue==-1,"Unknown / Non-Applicable",Revenue))

all_jobs$Company.Name <-  ifelse(all_jobs$Company.Name=="", "Not Applicable",all_jobs$Company.Name)

```

\newpage

## EDA

Since our goal is to see if the predictors that have an effect on the median outcome of the data related job post, I would like to get a glimpse at the distribution of the median income. And on the graph below, we can see most jobs are offering around 70k on the Glassdoor. I'd like to find out what predictors influence the income the most.

```{r echo=FALSE, fig.height=3.6, fig.width=5, fig.cap="Income Distribution", warning=FALSE}
ggplot(data=all_jobs, aes(x=median)) +
  geom_histogram(fill="white", color="black",bins=30) +
  labs(x = "Median expected income in thousands") 

```

First of all, I would like to find the relationship between the programming skills, such as R, Python, SQL, etc., and the median income posted by In the following graph, I am showing the skills mentioned in the job description in relation to the predictive median income from companies in different sizes and revenues.

```{r echo=FALSE, fig.height=3.6, fig.width=5, fig.cap="Positive Relation between Log Income and Skills by Size", warning=FALSE}
python_graph <- ggplot(data = all_jobs) + 
  aes(x = python, y = log(median)) + 
  geom_point(aes(color = factor(Size)), size = .6) + 
  geom_smooth(aes(color = factor(Size)), method = "lm", se = FALSE, formula = 'y ~ x') + 
  labs(title = "(a) Salary vs Python", x = "Requred python or not", y = "log(Salary)")

r_graph <- ggplot(data = all_jobs) + 
  aes(x = R, y = log(median)) + 
  geom_point(aes(color = factor(Size)), size = .6) + 
  geom_smooth(aes(color = factor(Size)), method = "lm", se = FALSE, formula = 'y ~ x') + 
  labs(title = "(a) Salary vs R", x = "Requred R or not", y = "log(Salary)")

spark_graph <- ggplot(data = all_jobs) + 
  aes(x = spark, y = log(median)) + 
  geom_point(aes(color = factor(Size)), size = .6) + 
  geom_smooth(aes(color = factor(Size)), method = "lm", se = FALSE, formula = 'y ~ x') + 
  labs(title = "(a) Salary vs spark", x = "Requred spark or not", y = "log(Salary)")

aws_graph <- ggplot(data = all_jobs) + 
  aes(x = aws, y = log(median)) + 
  geom_point(aes(color = factor(Size)), size = .6) + 
  geom_smooth(aes(color = factor(Size)), method = "lm", se = FALSE, formula = 'y ~ x') + 
  labs(title = "(a) Salary vs AWS skill", x = "Requred AWS or not", y = "log(Salary)")


grid.arrange(python_graph, r_graph, spark_graph, aws_graph, ncol=2)
```

As you can see in the graph, these skills are positively correlated with the log income. For example, the job description has the key word: python; unsurprisingly, jobs with Python mentioned in description tend to have a higher median income than those without.

However, there are also skills that are not as much related to the predictive median income or even has a negative relationship on the income. The SQL skill and Excel skill seem to not be a positive influence on the income.

```{r echo=FALSE, fig.height=2.5, fig.width=5, fig.cap="Non Positive Relation between Log Income and Skills", warning=FALSE}
sql_graph <- ggplot(data = all_jobs) + 
  aes(x = sql, y = log(median)) + 
  geom_point(aes(color = factor(Size)), size = .6) + 
  geom_smooth(aes(color = factor(Size)), method = "lm", se = FALSE, formula = 'y ~ x') + 
  labs(title = "(a) Salary vs SQL", x = "Requred SQL or not", y = "log(Salary)")

excel_graph <- ggplot(data = all_jobs) + 
  aes(x = excel, y = log(median)) + 
  geom_point(aes(color = factor(Size)), size = .6) + 
  geom_smooth(aes(color = factor(Size)), method = "lm", se = FALSE, formula = 'y ~ x') + 
  labs(title = "(a) Salary vs Excel", x = "Requred Excel or not", y = "log(Salary)")


grid.arrange(sql_graph, excel_graph, ncol=2)
```

I would also like to get the relationship between the continuous predictor, rating of the companies, and predictive income. There are multiple classifications on jobs provided by different companies. For example, the location of the job, the sector of the job, and the even companies' sizes and revenues. All of these classifications can be incorporated into the multilevel regression model. In the following graph, I am showing the rating of the company in relation to the predictive median income of the job posts for companies in different sizes and revenues.

```{r echo=FALSE, fig.height=3.6, fig.width=5, fig.cap="Relation between Log Income and Rating", warning=FALSE}
ggplot(data = all_jobs) + 
  aes(x = log(Rating+1), y = log(median)) + 
  geom_point(aes(color = factor(Size)), size = .6) + 
  geom_smooth(aes(color = factor(Size)), method = "lm", se = FALSE, formula = 'y ~ x') + 
  labs(title = "(a) Salary vs Rating", x = "log(average Rating)", y = "log(Salary)")
```

## Model fitting

First of all, by examine the data, the size and the revenue of the company seems to have some impact on the income. So I choose to apply the multilevel model fitting on the data. it is important to take the log on the large values of income and factorize the predictors with multiple categories, which then could be put into the model.

```{r include=FALSE}
log_jobs <- all_jobs %>% dplyr::select(Company.Name, Type.of.ownership, Location, Headquarters,Job.Title, Size, Industry,Sector, Revenue, median, Rating, R, python, aws, spark, sql, excel) %>% data.frame()

log_jobs$Company.Name <- as.factor(log_jobs$Company.Name)
log_jobs$Location <- factor(log_jobs$Location)
log_jobs$Type.of.ownership <- factor(log_jobs$Type.of.ownership)
log_jobs$Headquarters <- factor(log_jobs$Headquarters)
log_jobs$Job.Title <- as.factor(log_jobs$Job.Title)
log_jobs$Size <- factor(log_jobs$Size)
log_jobs$Industry <- factor(log_jobs$Industry)
log_jobs$Sector <- factor(log_jobs$Sector)
log_jobs$Revenue <- factor(log_jobs$Revenue)
log_jobs$median <- log(log_jobs$median + 1)
log_jobs$Rating <- log(log_jobs$Rating + 2)

```

Since many of the predictors are got from job descriptions, those are binary predictors: 1 for True and 0 for False. To fit these predictors into the model, they are fixed. Whereas, the `Location`, `Job.Title`, `Headquarters` , `Size`, and `Revenue` are can be random effects. I first try to fit the mixed model including all the fixed and random effects. However, some of the coefficient are not significant. As a result, I only leave some of the significant random effect with high random intercept variance over the total variance; particularly, I used `Location` and `Job.Title` for random effect.

```{r include=FALSE}
model1 <- lmerTest::lmer(median ~ Rating + R + python + aws + sql + spark + excel +  (1|Location) + (1 |Job.Title), data=log_jobs)

model2 <- lmerTest::lmer(median ~ Rating + R + python + aws + sql + spark + excel +  (1|Location) + (1|Job.Title), data=log_jobs)

model3 <- lmerTest::lmer(median ~ Rating + R + python + aws + sql + spark + excel +  (1|Location) + (1|Job.Title) + (1|Headquarters) , data=log_jobs)

model4 <- lmerTest::lmer(median ~ Rating + R + python + aws + sql + spark + excel +  (1|Location) + (1|Job.Title) + (1 | Size) , data=log_jobs)



anova(model1, model2, model3,model4)

anova(model1, model3)

summary(model1)
plot(model1)
plot(ranef(model1))
qqmath(model1)
anova(model1)

RandomEffects1 <- as.data.frame(VarCorr(model1))
ICC_between1 <- RandomEffects1[1,4]/(RandomEffects1[1,4]+RandomEffects1[3,4]) 
RandomEffects1
ICC_between1

```

```{r include=FALSE}
model1 <- lmerTest::lmer(median ~ Rating + R + python + aws + sql + spark + excel +  (1|Location) + (1|Job.Title) , data=log_jobs)

```

```{r eval=FALSE}
model1 <- lmerTest::lmer(median ~ Rating + R + python + aws + sql + spark + excel +  (1|Location) + (1|Job.Title) , data=log_jobs)

```

From the summary table and graph of fixed effect, we can consider the predictor is statistically significance at $\alpha=0.05$ level. In this case, the coefficients for `python`, `aws`, `sql`, `spark`, and `excel` all have p-values less than 0.05, indicating that they are significant predictors in the model. The coefficient for `Rating` and `R` does not have a significant p-value, indicating that it is not a significant predictor in this model.

```{r echo=FALSE}
coef(summary(model1)) %>% kable() %>% kable_styling()

```

```{r echo=FALSE, fig.height=3.6, fig.width=5, fig.cap="Fixed Effect of Median Income Model"}
plotFEsim(FEsim(model1, n.sims = 100), level = 0.9, stat = 'median', intercept = FALSE)
```

From the graph above, we can see the significance of fixed effect on all predictors. As you can see, there are 2 predictors are not as significant as others: the `R` and `Rating`.

```{r echo=FALSE}
RandomEffects1 <- as.data.frame(VarCorr(model1))
RandomEffects1 %>% kable() %>% kable_styling()

```

As for the random effect, as you can see, both `Job.Title` and `Location` has a high proportion relative to the residual. So the ICC score will be high.

```{r echo=FALSE}
head(round(ranef(model1)$Location, digits = 2))  %>% kable() %>% kable_styling()
head(round(ranef(model1)$Job.Title, digits = 2))  %>% kable() %>% kable_styling()
```

```{r echo=FALSE, fig.cap="Posterior Predictive Check"}
performance::posterior_predictive_check(object = model1, iterations = 100)

```

From the figure 6, we can see a pretty close posterior predictive curve compared to the actual ones.

## Results

### Interpretation

First, we can fit the formula with only the fixed effect: $$log(median+1)=4.31 + 0.08 \times python +  0.02 \times aws - 0.01 \times sql + 0.08 \times spark -0.02 \times excel $$ Different Job Title and Working location will also have an impact on the median income. And in the formula, the main change is on the interceptions. If we take the Sr. Data Engineer as the job title and the working location is in California, the formula will become: $$ log(median+1)=4.37 + 0.08 \times python +  0.02 \times aws - 0.01 \times sql + 0.08 \times spark -0.02 \times excel  $$ In both formula, Rating and R are dropped due to its insignificant p-valus. The formula can be interpret as for a Sr. Data Engineer worked in California, the median income is $e^{4.31}-1= 73.44$ thousands. The coefficient for `python` has a weight of 0.08, which means that if `python` is addressed in the job description, the averaged median income is $e^{0.08}= 1.08$ times higher than those isn't. The coefficient for `spark` is also 0.08 and so it can be interpreted the same as `python.` `aws` has a coefficient of 0.02, which means that if `aws` is addressed in the job description, the averaged median income is $e^{0.02}= 1.02$ times higher. The coefficients for `sql` and `excel` are negative, which means that job descriptions with SQL and excel mentioned on average have lower median income than those aren't. Basically, jobs with SQL will have 1% lower median income than those aren't and those with excel are 2% lower.

## Model Check

On the left is the residual plot and right is the residual Q-Q plot. On the residual plot, we can see the points are kept in the -2 to 2 range and the distribution of points are relatively random. On the residual Q-Q plot, we can see the residual is roughly randomly distributed except for the tails. So we can accept the normal distribution assumption.

```{r echo=FALSE, fig.height=2.5, fig.width=5}
residual_1 <- plot(model1)
residual_2 <- qqmath(model1)
grid.arrange(residual_1, residual_2, ncol=2)
```

## Discussion

In this project, I used the multilevel model to find out the elements that have an impact on the predictive median income for the data science related jobs. And the model take account in two group effects: the job's title and job's location. In short, the fixed effects aren't all as significant as I thought at the beginning and I even find negative correlation between the predictors and outcome which I've never expected before. So the Rating of the company and the R programming skills are not statistically significant to the income. While python, aws, and spark is positively correlated to the income, SQL and excel has negatively correlation to income. As for the random effects, they are reasonably identify the difference between jobs in different titles and locations by changing the intercepts of the model.

However, there are some limitations in my project as well. First and foremost, most of my predictors are extract from the job descriptions consist of a big chunk of text. Although it will be precise for some distinctive words such as python and spark, getting the R programming skill out of the text is hard and imprecise. So I might miss some predictor which leads to an underestimate of the significance.

\newpage

## References

[1] https://quantdev.ssri.psu.edu/sites/qdev/files/RBootcamp_MLMInteractions_2019_0820_Final2.html

[2] https://github.com/BU-Franky/MA678_midterm



## Apendix

```{r echo=FALSE, fig.height=3.6, fig.width=5, fig.cap="Relation between Log Income and Rating by Revenue", warning=FALSE}
ggplot(data = all_jobs) + 
  aes(x = log(Rating+1), y = log(median)) + 
  geom_point(aes(color = factor(Revenue)), size = .6) + 
  geom_smooth(aes(color = factor(Revenue)), method = "lm", se = FALSE, formula = 'y ~ x') + 
  labs(title = "(a) Salary vs Rating", x = "log(average Rating)", y = "log(Salary)")
```

```{r echo=FALSE, fig.height=3.6, fig.width=5}
ggplot(data=all_jobs, aes(x=python,y=median)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Rating") + ylab("median for expected income") + 
  facet_wrap( ~ Size) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))
```
